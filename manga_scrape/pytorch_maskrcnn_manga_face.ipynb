{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_path, annot_path, classes, transforms=None, is_train=True, image_mode='RGB'):\n",
    "        self.images_path = images_path\n",
    "        self.annot_path = annot_path\n",
    "        self.transforms = transforms\n",
    "        self.classes = classes\n",
    "        self.is_train = is_train\n",
    "        self.image_mode = image_mode  # Add image_mode argument\n",
    "        \n",
    "        self.files = [file for file in os.listdir(images_path)]\n",
    "        random.shuffle(self.files)\n",
    "        \n",
    "        split = 0.7    \n",
    "        split_index = int(np.floor(len(self.files) * split))\n",
    "        \n",
    "        if self.is_train:\n",
    "            self.files = self.files[:split_index]\n",
    "        else:\n",
    "            self.files = self.files[split_index:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files[idx]\n",
    "        image_id = filename.split('.')[0]\n",
    "        img_path = os.path.join(self.images_path, filename)\n",
    "        ann_path = os.path.join(self.annot_path, image_id + '.xml')\n",
    "\n",
    "        try:\n",
    "            # Open the image based on the specified mode\n",
    "            if self.image_mode == 'RGB':\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            else:\n",
    "                img = Image.open(img_path).convert(\"L\")\n",
    "            \n",
    "            boxes, labels, masks = self.extract_boxes(ann_path, img.size)\n",
    "\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"masks\"] = masks\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "            \n",
    "            return img, target, img_path  # Return img_path for visualization\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_boxes(self, filename, image_size):\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        \n",
    "        width, height = image_size\n",
    "        \n",
    "        for obj in root.findall('.//object'):\n",
    "            label = obj.find('name').text.lower().strip()\n",
    "            labels.append(self.classes.index(label))\n",
    "            \n",
    "            box = obj.find('bndbox')\n",
    "            xmin = int(float(box.find('xmin').text))\n",
    "            ymin = int(float(box.find('ymin').text))\n",
    "            xmax = int(float(box.find('xmax').text))\n",
    "            ymax = int(float(box.find('ymax').text))\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "            mask = Image.new('L', (width, height), 0)\n",
    "            mask_draw = ImageDraw.Draw(mask)\n",
    "            mask_draw.rectangle([xmin, ymin, xmax, ymax], outline=1, fill=1)\n",
    "            masks.append(np.array(mask))\n",
    "        \n",
    "        return boxes, labels, masks\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [torchvision.transforms.ToTensor()]\n",
    "    if train:\n",
    "        transforms.append(torchvision.transforms.RandomHorizontalFlip(0.5))\n",
    "    return torchvision.transforms.Compose(transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Paths to your images and annotations\n",
    "images_path = r'C:\\Users\\brunolopez\\Downloads\\komi_analysis_pascal\\JPEGImages'\n",
    "annot_path = r'C:\\Users\\brunolopez\\Downloads\\komi_analysis_pascal\\Annotations'\n",
    "classes = ['background', 'face']  # Add more classes as needed\n",
    "\n",
    "# Create datasets and data loaders\n",
    "dataset = CustomDataset(images_path, annot_path, classes, transforms=get_transform(train=True), image_mode='L')\n",
    "dataset_test = CustomDataset(images_path, annot_path, classes, transforms=get_transform(train=False), image_mode='L')\n",
    "\n",
    "# Split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# Define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Get the Mask R-CNN model\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = len(classes)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    for images, targets, _ in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Train for 10 epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"komi_maskrcnn.pth\")\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "for images, targets, img_paths in data_loader_test:\n",
    "    images = list(img.to(device) for img in images)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    # Access the image_mode attribute from the original dataset\n",
    "    image_mode = data_loader_test.dataset.dataset.image_mode\n",
    "\n",
    "    for i, output in enumerate(outputs):\n",
    "        img_path = img_paths[i]\n",
    "        original_img = Image.open(img_path).convert(\"L\")  # Open original image for visualization\n",
    "        mask_img = np.zeros_like(np.array(original_img))\n",
    "\n",
    "        # Create mask image from the output masks\n",
    "        for mask in output['masks']:\n",
    "            mask = mask[0, :, :].cpu().numpy()\n",
    "            mask_img = np.maximum(mask_img, mask * 255)\n",
    "        \n",
    "        # Plot side-by-side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # Plot original image with bounding boxes\n",
    "        ax1.imshow(original_img, cmap='gray')\n",
    "        for box in output['boxes']:\n",
    "            box = box.cpu().numpy()\n",
    "            ax1.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor='red', linewidth=2))\n",
    "        ax1.set_title(\"Original Image with Annotations\")\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Plot mask image with bounding boxes\n",
    "        ax2.imshow(mask_img, cmap='gray')\n",
    "        for box in output['boxes']:\n",
    "            box = box.cpu().numpy()\n",
    "            ax2.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor='red', linewidth=2))\n",
    "        ax2.set_title(\"Mask with Annotations\")\n",
    "        ax2.axis('off')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_path, annot_path, classes, transforms=None, is_train=True, image_mode='RGB'):\n",
    "        self.images_path = images_path\n",
    "        self.annot_path = annot_path\n",
    "        self.transforms = transforms\n",
    "        self.classes = classes\n",
    "        self.is_train = is_train\n",
    "        self.image_mode = image_mode  # Add image_mode argument\n",
    "        \n",
    "        self.files = [file for file in os.listdir(images_path)]\n",
    "        random.shuffle(self.files)\n",
    "        \n",
    "        split = 0.7    \n",
    "        split_index = int(np.floor(len(self.files) * split))\n",
    "        \n",
    "        if self.is_train:\n",
    "            self.files = self.files[:split_index]\n",
    "        else:\n",
    "            self.files = self.files[split_index:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files[idx]\n",
    "        image_id = filename.split('.')[0]\n",
    "        img_path = os.path.join(self.images_path, filename)\n",
    "        ann_path = os.path.join(self.annot_path, image_id + '.xml')\n",
    "\n",
    "        try:\n",
    "            # Open the image based on the specified mode\n",
    "            if self.image_mode == 'RGB':\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            else:\n",
    "                img = Image.open(img_path).convert(\"L\")\n",
    "            \n",
    "            boxes, labels, masks = self.extract_boxes(ann_path, img.size)\n",
    "\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"masks\"] = masks\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "            \n",
    "            return img, target, img_path  # Return img_path for visualization\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_boxes(self, filename, image_size):\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        \n",
    "        width, height = image_size\n",
    "        \n",
    "        for obj in root.findall('.//object'):\n",
    "            label = obj.find('name').text.lower().strip()\n",
    "            labels.append(self.classes.index(label))\n",
    "            \n",
    "            box = obj.find('bndbox')\n",
    "            xmin = int(float(box.find('xmin').text))\n",
    "            ymin = int(float(box.find('ymin').text))\n",
    "            xmax = int(float(box.find('xmax').text))\n",
    "            ymax = int(float(box.find('ymax').text))\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "            mask = Image.new('L', (width, height), 0)\n",
    "            mask_draw = ImageDraw.Draw(mask)\n",
    "            mask_draw.rectangle([xmin, ymin, xmax, ymax], outline=1, fill=1)\n",
    "            masks.append(np.array(mask))\n",
    "        \n",
    "        return boxes, labels, masks\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [torchvision.transforms.ToTensor()]\n",
    "    if train:\n",
    "        transforms.append(torchvision.transforms.RandomHorizontalFlip(0.5))\n",
    "    return torchvision.transforms.Compose(transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    \n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "# Paths to your images and annotations\n",
    "images_path = r'C:\\Users\\brunolopez\\Downloads\\komi_analysis_pascal\\JPEGImages'\n",
    "annot_path = r'C:\\Users\\brunolopez\\Downloads\\komi_analysis_pascal\\Annotations'\n",
    "classes = ['background', 'face']  # Add more classes as needed\n",
    "\n",
    "# Create datasets and data loaders\n",
    "dataset = CustomDataset(images_path, annot_path, classes, transforms=get_transform(train=True), image_mode='L')\n",
    "dataset_test = CustomDataset(images_path, annot_path, classes, transforms=get_transform(train=False), image_mode='L')\n",
    "\n",
    "# Split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# Define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Get the Mask R-CNN model\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = len(classes)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    for images, targets, _ in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Train for 10 epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"komi_maskrcnn.pth\")\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "for images, targets, img_paths in data_loader_test:\n",
    "    images = list(img.to(device) for img in images)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    # Access the image_mode attribute from the original dataset\n",
    "    image_mode = data_loader_test.dataset.dataset.image_mode\n",
    "\n",
    "    for i, output in enumerate(outputs):\n",
    "        img_path = img_paths[i]\n",
    "        original_img = Image.open(img_path).convert(\"L\")  # Open original image for visualization\n",
    "        mask_img = np.zeros_like(np.array(original_img))\n",
    "\n",
    "        gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "\n",
    "        # Create mask image from the output masks\n",
    "        for mask in output['masks']:\n",
    "            mask = mask[0, :, :].cpu().numpy()\n",
    "            mask_img = np.maximum(mask_img, mask * 255)\n",
    "        \n",
    "        # Plot side-by-side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # Plot original image with bounding boxes\n",
    "        ax1.imshow(original_img, cmap='gray')\n",
    "        for box in output['boxes']:\n",
    "            box = box.cpu().numpy()\n",
    "            ious = [compute_iou(box, gt_box) for gt_box in gt_boxes]\n",
    "            max_iou = max(ious) if ious else 0\n",
    "            color = cm.viridis(max_iou)[:3]  # Get color from viridis colormap\n",
    "            ax1.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor=color, linewidth=2))\n",
    "        ax1.set_title(\"Original Image with Annotations\")\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Plot mask image with bounding boxes\n",
    "        ax2.imshow(mask_img, cmap='gray')\n",
    "        for box in output['boxes']:\n",
    "            box = box.cpu().numpy()\n",
    "            ious = [compute_iou(box, gt_box) for gt_box in gt_boxes]\n",
    "            max_iou = max(ious) if ious else 0\n",
    "            color = cm.viridis(max_iou)[:3]  # Get color from viridis colormap\n",
    "            ax2.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor=color, linewidth=2))\n",
    "        ax2.set_title(\"Mask with Annotations\")\n",
    "        ax2.axis('off')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
